{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# developer Mujtaba Ghulami for learn multihead attention model and sample PositionalEncoding"
      ],
      "metadata": {
        "id": "G3jRG4I5mYtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets transformers\n",
        "!pip install torchinfo\n",
        "!pip install torchviz"
      ],
      "metadata": {
        "id": "J59QEk5bmZnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "#from torchviz import make_dot\n",
        "#from torchinfo import summary\n",
        "from transformers import GPT2Tokenizer\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import pandas as pd\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "hgDr4pVCmZuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJu2SucemL9Q"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Rotary Position Embedding (RoPE)\n",
        "class RotaryPositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=8192):\n",
        "        super(RotaryPositionalEmbedding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        # Compute theta values for rotation - only for half the dimensions\n",
        "        inv_freq = 1.0 / (10000 ** (torch.arange(0, d_model, 2).float() / d_model))\n",
        "        self.register_buffer('inv_freq', inv_freq)\n",
        "\n",
        "    def forward(self, x, seq_len=None):\n",
        "        if seq_len is None:\n",
        "            seq_len = x.size(1)\n",
        "\n",
        "        # Create position indices\n",
        "        t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n",
        "        # Compute frequencies: outer product of positions and inv_freq\n",
        "        freqs = torch.einsum('i,j->ij', t, self.inv_freq)  # (seq_len, d_model//2)\n",
        "\n",
        "        return freqs.cos(), freqs.sin()\n",
        "\n",
        "def apply_rotary_pos_emb(x, cos, sin):\n",
        "    # x shape: (batch, num_heads, seq_len, head_dim)\n",
        "    # cos, sin shape: (seq_len, head_dim//2)\n",
        "\n",
        "    # Split x into first half and second half\n",
        "    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n",
        "\n",
        "    # Expand cos and sin to match x dimensions\n",
        "    # cos and sin are (seq_len, head_dim//2), need to be (1, 1, seq_len, head_dim//2)\n",
        "    cos = cos[None, None, :, :]  # (1, 1, seq_len, head_dim//2)\n",
        "    sin = sin[None, None, :, :]  # (1, 1, seq_len, head_dim//2)\n",
        "\n",
        "    # Rotate: [x1, x2] * [[cos, -sin], [sin, cos]]\n",
        "    return torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
        "\n",
        "\n",
        "# Causal Self-Attention with RoPE\n",
        "class MatrixModelWithRoPE(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, max_len=8192):\n",
        "        super(MatrixModelWithRoPE, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        # Linear projections for Q, K, V\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        # RoPE\n",
        "        self.rope = RotaryPositionalEmbedding(self.head_dim, max_len)\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(ff_dim, embed_dim)\n",
        "        )\n",
        "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
        "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch, seq_len, embed_dim)\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # Project to Q, K, V\n",
        "        q = self.q_proj(x)  # (batch, seq_len, embed_dim)\n",
        "        k = self.k_proj(x)\n",
        "        v = self.v_proj(x)\n",
        "\n",
        "        # Reshape for multi-head attention: (batch, seq_len, num_heads, head_dim)\n",
        "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose for attention computation: (batch, num_heads, seq_len, head_dim)\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        # Apply RoPE to queries and keys\n",
        "        cos, sin = self.rope(q, seq_len)\n",
        "        q = apply_rotary_pos_emb(q, cos, sin)\n",
        "        k = apply_rotary_pos_emb(k, cos, sin)\n",
        "\n",
        "        # Compute attention scores\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        # Apply causal mask\n",
        "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()\n",
        "        scores = scores.masked_fill(causal_mask, float('-inf'))\n",
        "\n",
        "        # Apply softmax\n",
        "        attn_weights = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        # Apply attention to values\n",
        "        attn_output = torch.matmul(attn_weights, v)  # (batch, num_heads, seq_len, head_dim)\n",
        "\n",
        "        # Concatenate heads: (batch, seq_len, embed_dim)\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
        "\n",
        "        # Output projection\n",
        "        attn_output = self.o_proj(attn_output)\n",
        "\n",
        "        # Residual connection and layer normalization\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        # Feed-forward network\n",
        "        ffn_output = self.ffn(out1)\n",
        "\n",
        "        # Residual connection and layer normalization\n",
        "        output = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# Full Model with RoPE\n",
        "class MatrixGPT_rope(nn.Module):\n",
        "    def __init__(self, vocab_size, max_length, embed_dim, num_layers,\n",
        "                 num_heads, ff_dim):\n",
        "        super(MatrixGPT_rope, self).__init__()\n",
        "        self.max_length = max_length\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Embedding layer (no positional encoding needed with RoPE)\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # List of Causal Self-Attention layers with RoPE\n",
        "        self.MatrixModel_layers = nn.ModuleList([\n",
        "            MatrixModelWithRoPE(embed_dim, num_heads, ff_dim, max_length)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.output_layer = nn.Linear(self.embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch, seq_len)\n",
        "        x = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
        "        # No separate positional encoding - RoPE is applied in attention\n",
        "\n",
        "        for attn in self.MatrixModel_layers:\n",
        "            x = attn(x)\n",
        "\n",
        "        # Output layer\n",
        "        logits = self.output_layer(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qZ_8-GETylz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50259\n",
        "max_length = 1024\n",
        "embed_dim = 1536\n",
        "num_layers = 16   # Increase depth for better representation\n",
        "num_heads = 8\n",
        "ff_dim = 6144\n",
        "model_path= \"MatrixGPT_RoPE.pth\" #\"/kaggle/input/matrix/MatrixGPT_RoPE.pth\" #\"/content/drive/MyDrive/brain_p/MatrixGPT_RoPE.pth\" # \"/kaggle/working/MatrixGPT_RoPE.pth\"\n",
        "save_path = \"MatrixGPT_RoPE.pth\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "QI-OkIf0mZ2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "model = MatrixGPT_rope(\n",
        "    vocab_size=vocab_size,\n",
        "    max_length=max_length,\n",
        "    embed_dim=embed_dim,\n",
        "    num_layers=num_layers,\n",
        "    num_heads=num_heads,\n",
        "    ff_dim=ff_dim,\n",
        ")"
      ],
      "metadata": {
        "id": "Ncih-e72mmgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "if torch.cuda.device_count() > 1:\n",
        "    # print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    # model = nn.DataParallel(model)\n",
        "    pass\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "bn07CGypmm3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# More efficient for multi-GPU\n",
        "# from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "# # Setup (more complex, but worth it)\n",
        "# torch.distributed.init_process_group(backend='nccl')\n",
        "# local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
        "# model = model.to(local_rank)\n",
        "# model = DDP(model, device_ids=[local_rank])\n",
        "\n",
        "\n",
        "\n",
        "# if isinstance(model, nn.DataParallel):\n",
        "#     original_model = model.module\n",
        "# else:\n",
        "#     original_model = model\n",
        "\n",
        "# # Save\n",
        "# torch.save(model.module.state_dict() if isinstance(model, nn.DataParallel)\n",
        "#            else model.state_dict(), \"checkpoint.pth\")"
      ],
      "metadata": {
        "id": "fyqr-d00mm6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Define header tokens\n",
        "START_HEADER = \"<|startheader|>\"\n",
        "END_HEADER = \"<|endheader|>\"\n",
        "\n",
        "# Load the dataset\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "# Set up padding and end-of-text token\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Use EOS token as padding\n",
        "\n",
        "# Add special tokens: ensure EOS and header tokens are added\n",
        "special_tokens_dict = {\n",
        "    \"eos_token\": \"<|endoftext|>\",\n",
        "    \"additional_special_tokens\": [START_HEADER, END_HEADER]\n",
        "}\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "def _return_header(message) -> str:\n",
        "    role = message.get(\"from\", \"\")\n",
        "    if role == \"system\":\n",
        "        return \"system:\"\n",
        "    elif role == \"gpt\":\n",
        "        return \"assistant:\"\n",
        "    elif role == \"human\":\n",
        "        return \"user:\"\n",
        "    return \"unknown:\"\n",
        "\n",
        "def encode_header(message):\n",
        "    header = _return_header(message)\n",
        "    # Wrap the header text with start and end header tokens\n",
        "    return f\"{START_HEADER}{header}{END_HEADER}\"\n",
        "\n",
        "def encode_message(message) -> str:\n",
        "    text = encode_header(message)\n",
        "    text += message[\"value\"].strip()\n",
        "    text += \"<|endoftext|>\"  # Append the correct end-of-text token\n",
        "    return text\n",
        "\n",
        "def encode_dialog_prompt(dialog):\n",
        "    # Concatenate all messages in the dialog into one string.\n",
        "    return \"\".join(encode_message(message) for message in dialog)\n",
        "\n",
        "def hermes_ins(batch):\n",
        "    # Encode the conversation in each batch item\n",
        "    texts = [encode_dialog_prompt(item['conversations']) for item in batch]\n",
        "    tokenized = tokenizer(\n",
        "        texts,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,  # You may change padding behavior if desired\n",
        "        truncation=True,\n",
        "        max_length=max_length + 1  # Increased max_length by 1 to account for labels\n",
        "    )\n",
        "    input_ids = tokenized[\"input_ids\"].long()\n",
        "    inputs = input_ids[:, :-1]\n",
        "    labels = input_ids[:, 1:]\n",
        "    return {\"input_ids\": inputs, \"labels\": labels, \"text\": texts}\n",
        "\n",
        "# Create DataLoader\n"
      ],
      "metadata": {
        "id": "73MR0WfYmm9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#!wget https://huggingface.co/datasets/teknium/OpenHermes-2.5/resolve/main/openhermes2_5.json"
      ],
      "metadata": {
        "id": "HUPAAqjfmm_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the JSON data using pandas\n",
        "#df = pd.read_json(\"openhermes2_5.json\")\n",
        "\n",
        "# Create a Dataset from the pandas DataFrame\n",
        "#OpenHermes = Dataset.from_pandas(df)"
      ],
      "metadata": {
        "id": "dZWGzCJvmnCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OpenHermes = load_dataset(\"teknium/OpenHermes-2.5\", split='train')\n",
        "hermes_instruct = DataLoader(OpenHermes, batch_size=1, shuffle=True, collate_fn=hermes_ins)"
      ],
      "metadata": {
        "id": "_l-JKjVqmnFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_all(model, optimizer, loss, save_path):\n",
        "    # Handle wrapped models (DDP/DataParallel)\n",
        "    model_state = model.module.state_dict() if hasattr(model, 'module') else model.state_dict()\n",
        "\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model_state,\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "    }\n",
        "    torch.save(checkpoint, save_path)\n",
        "    time.sleep(7)"
      ],
      "metadata": {
        "id": "tQoC-zDymnIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize once (don't re-initialize if already done)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)"
      ],
      "metadata": {
        "id": "6bCHP98CurzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture\n",
        "# # Loading\n",
        "# checkpoint = torch.load(model_path, map_location=device)\n",
        "\n",
        "# # Handle wrapped models when loading\n",
        "# if hasattr(model, 'module'):\n",
        "#     model.module.load_state_dict(checkpoint['model_state_dict'])\n",
        "# else:\n",
        "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "# loss = checkpoint['loss']\n",
        "# del checkpoint"
      ],
      "metadata": {
        "id": "ZtFlW9PlnIYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#str(loss.item())\n",
        "len(hermes_instruct)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xuDm37vnIex",
        "outputId": "e326513b-ef63-49ab-e5bf-251d0488c0e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1001551"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_cosine_schedule_with_warmup\n",
        "import torch.nn.utils as nn_utils\n",
        "\n",
        "\n",
        "# Setup\n",
        "num_epochs = 10\n",
        "save_every = 3500\n",
        "total_steps = num_epochs * len(hermes_instruct)\n",
        "warmup_steps = int(0.1 * total_steps)\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "# Gradient accumulation settings\n",
        "accum_steps = 16\n",
        "effective_batch_size = 1 * accum_steps\n",
        "print(f\"Simulating effective batch size: {effective_batch_size}\")\n",
        "\n",
        "# Training loop\n",
        "global_step = 0\n",
        "saved = 0\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()  # Clear at start of epoch\n",
        "\n",
        "    for batch_idx, batch in enumerate(hermes_instruct):\n",
        "        inputs = batch[\"input_ids\"].to(device)\n",
        "        targets = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "\n",
        "        # Scale loss for gradient accumulation\n",
        "        loss = loss / accum_steps\n",
        "        loss.backward()  # Accumulate gradients\n",
        "\n",
        "        # Perform optimization step after accum_steps batches\n",
        "        if (batch_idx + 1) % accum_steps == 0:\n",
        "            # Clip gradients before optimizer step\n",
        "            nn_utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Display unscaled loss\n",
        "            actual_loss = loss.item() * accum_steps\n",
        "            current_lr = scheduler.get_last_lr()[0]\n",
        "            print(f\"\\rEpoch: {epoch+1}/{num_epochs} | \"\n",
        "                  f\"Batch: {batch_idx+1}/{len(hermes_instruct)} | \"\n",
        "                  f\"Loss: {actual_loss:.4f} | \"\n",
        "                  f\"LR: {current_lr:.2e} | \"\n",
        "                  f\"Saved: {saved}\", end=\"\")\n",
        "\n",
        "        # Increment global step counter\n",
        "        global_step += 1\n",
        "\n",
        "        # Save checkpoint every save_every steps\n",
        "        if global_step % save_every == 0:\n",
        "            # Apply any remaining accumulated gradients before saving\n",
        "            if (batch_idx + 1) % accum_steps != 0:\n",
        "                nn_utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            actual_loss = loss.item() * accum_steps\n",
        "            save_all(model, optimizer, f\"{actual_loss:.4f}\",save_path)\n",
        "            saved += 1\n",
        "            print(f\"\\n✓ Model saved at step {global_step}\")\n",
        "\n",
        "            if saved % 3 == 0:\n",
        "                # upload()  # Optional cloud backup\n",
        "                pass\n",
        "\n",
        "    # Handle remaining gradients at end of epoch\n",
        "    if (batch_idx + 1) % accum_steps != 0:\n",
        "        nn_utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1} completed\")"
      ],
      "metadata": {
        "id": "63xxWNUEnIno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import get_cosine_schedule_with_warmup\n",
        "# import torch.nn.utils as nn_utils\n",
        "\n",
        "# # Setup\n",
        "# num_epochs = 10\n",
        "# save_every = 3500\n",
        "# total_steps = num_epochs * len(hermes_instruct)\n",
        "# warmup_steps = int(0.1 * total_steps)\n",
        "\n",
        "# scheduler = get_cosine_schedule_with_warmup(\n",
        "#     optimizer,\n",
        "#     num_warmup_steps=warmup_steps,\n",
        "#     num_training_steps=total_steps\n",
        "# )\n",
        "\n",
        "# # Training\n",
        "# global_step = 0\n",
        "# saved = 0\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     epoch_loss = 0\n",
        "\n",
        "#     for batch_idx, batch in enumerate(hermes_instruct):\n",
        "#         inputs = batch[\"input_ids\"].to(device)\n",
        "#         targets = batch[\"labels\"].to(device)\n",
        "\n",
        "#         # Forward pass\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(inputs)\n",
        "#         loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "\n",
        "#         # Backward pass\n",
        "#         loss.backward()\n",
        "#         nn_utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "#         optimizer.step()\n",
        "#         scheduler.step()\n",
        "\n",
        "#         # Track metrics\n",
        "#         global_step += 1\n",
        "#         epoch_loss += loss.item()\n",
        "\n",
        "#         # Display progress\n",
        "#         print(f\"\\rEpoch: {epoch+1}/{num_epochs} | \"\n",
        "#               f\"Batch: {batch_idx+1}/{len(hermes_instruct)} | \"\n",
        "#               f\"Loss: {loss.item():.4f} | \"\n",
        "#               f\"LR: {scheduler.get_last_lr()[0]:.2e} | \"\n",
        "#               f\"Saved: {saved}\", end=\"\")\n",
        "\n",
        "#         # Periodic saving\n",
        "#         if global_step % save_every == 0:\n",
        "#             save_all(model, optimizer, loss,save_path)\n",
        "#             saved += 1\n",
        "#             print(f\"\\n✓ Model saved at step {global_step}\")\n",
        "\n",
        "#             if saved % 3 == 0:\n",
        "#                 # upload()  # Optional cloud upload\n",
        "#                 pass\n",
        "\n",
        "#     # Epoch summary\n",
        "#     avg_loss = epoch_loss / len(hermes_instruct)\n",
        "#     print(f\"\\nEpoch {epoch+1} completed | Avg Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "4CcVSSo8nQpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_sampling(logits, k):\n",
        "    \"\"\"\n",
        "    Select the next token using top-k sampling.\n",
        "    Args:\n",
        "        logits (Tensor): Logits for the current token with shape [vocab_size].\n",
        "        k (int): The number of top tokens to sample from.\n",
        "    Returns:\n",
        "        int: The token id sampled from the top-k distribution.\n",
        "    \"\"\"\n",
        "    # Apply softmax to get probabilities.\n",
        "    probabilities = F.softmax(logits, dim=-1)\n",
        "    # Get the top-k token ids and their probabilities.\n",
        "    topk_probs, topk_indices = torch.topk(probabilities, k)\n",
        "    # Normalize the top-k probabilities.\n",
        "    topk_probs = topk_probs / torch.sum(topk_probs)\n",
        "    # Sample one token id from the top-k distribution.\n",
        "    next_token_id = torch.multinomial(topk_probs, 1).item()\n",
        "    # Get the corresponding token id from topk_indices.\n",
        "    return topk_indices[next_token_id].item()\n",
        "\n",
        "def generate_text_k(model, tokenizer, input_text,device, max_length=50, k=10):\n",
        "    model.eval()\n",
        "    # Tokenize the input text.\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "    generated = input_ids.tolist()[0]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            # Use only the last token as input along with the previous hidden state.\n",
        "            input_token = torch.tensor([[generated[-1]]]).to(device)\n",
        "            logits = model(input_token)\n",
        "            # Get logits for the last token (shape: [1, 1, vocab_size]) and remove unneeded dimensions.\n",
        "            next_token_logits = logits[:, -1, :].squeeze(0) # Corrected indexing\n",
        "\n",
        "            # Sample the next token using top-k sampling.\n",
        "            next_token_id = top_k_sampling(next_token_logits, k)\n",
        "            generated.append(next_token_id)\n",
        "\n",
        "            # Optionally, stop generation if the end-of-sequence token is generated.\n",
        "            if tokenizer.eos_token_id is not None and next_token_id == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    # Decode the complete generated token list.\n",
        "    generated_text = tokenizer.decode(generated, skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "def generate_text(model, tokenizer, input_text,device, max_length=50):\n",
        "    model.eval()\n",
        "    # Tokenize the input text.\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "    generated = input_ids.tolist()[0]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            # Provide the complete sequence each time to help the model consider context.\n",
        "            input_ids_tensor = torch.tensor([generated]).to(device)\n",
        "\n",
        "            # Get probability distribution for the next token.\n",
        "            logits = model(input_ids_tensor)\n",
        "            last_token_logits = logits[:, -1, :]  # shape (batch, vocab_size) # Corrected indexing\n",
        "            probabilities = torch.softmax(last_token_logits, dim=-1)\n",
        "            # Greedy sampling: choose the token with the highest probability.\n",
        "            next_token_id = torch.argmax(probabilities, dim=-1).item()\n",
        "            generated.append(next_token_id)\n",
        "\n",
        "            # Stop generation if the EOS token is produced.\n",
        "            if next_token_id == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    generated_text = tokenizer.decode(generated, skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "class TopPTextGenerator:\n",
        "    \"\"\"\n",
        "    A class to perform text generation using nucleus (top-p) sampling.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, tokenizer, top_p=0.9, temperature=1.0, device=None):\n",
        "        \"\"\"\n",
        "        model: PyTorch module that returns logits of shape [batch_size, seq_length, vocab_size]\n",
        "        tokenizer: A tokenizer with encode/decode methods and an eos_token_id attribute.\n",
        "        top_p: The cumulative probability threshold for nucleus sampling.\n",
        "        temperature: A factor to control randomness; higher values increase randomness.\n",
        "        device: torch.device to use.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.top_p = top_p\n",
        "        self.temperature = temperature\n",
        "        self.device = device\n",
        "\n",
        "    def nucleus_sampling(self, logits):\n",
        "        \"\"\"\n",
        "        Applies nucleus (top-p) filtering to the logits.\n",
        "        logits: Tensor of shape [vocab_size] representing logits for the next token.\n",
        "        Returns the logits with values filtered out that do not belong to the top-p cumulative distribution.\n",
        "        \"\"\"\n",
        "        # Apply temperature scaling\n",
        "        logits = logits / self.temperature\n",
        "\n",
        "        # Compute probabilities from logits\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # Sort the probabilities in descending order\n",
        "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "\n",
        "        # Compute cumulative probabilities of the sorted tensor\n",
        "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "        # Create a mask to filter out tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probs > self.top_p\n",
        "\n",
        "        # Shift the mask one token to the right to keep the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        # Prepare an output copy of logits to modify\n",
        "        filtered_logits = logits.clone()\n",
        "        # Get the indices to remove from the sorted token indices\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "        filtered_logits[indices_to_remove] = -float('Inf')\n",
        "        return filtered_logits\n",
        "\n",
        "    def generate(self, prompt, seq_len=50):\n",
        "        \"\"\"\n",
        "        Generates text conditioned on a prompt.\n",
        "\n",
        "        prompt: Starting text string.\n",
        "        seq_len: Maximum number of tokens to generate.\n",
        "        Returns the generated text string.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        # Encode the prompt using the GPT-2 tokenizer.\n",
        "        token_ids = self.tokenizer.encode(prompt)\n",
        "        input_ids = torch.tensor(token_ids, dtype=torch.long, device=self.device).unsqueeze(0)  # shape: [1, seq_length]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(seq_len):\n",
        "                # Get logits from the model (assuming model returns logits for every token)\n",
        "                logits = self.model(input_ids)  # shape: [1, current_seq_len, vocab_size]\n",
        "                next_token_logits = logits[0, -1, :]  # shape: [vocab_size] # Corrected indexing\n",
        "\n",
        "                # Apply nucleus sampling filtering to logits\n",
        "                filtered_logits = self.nucleus_sampling(next_token_logits)\n",
        "\n",
        "                # Convert filtered logits to probabilities and sample the next token\n",
        "                probs = F.softmax(filtered_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "                # Append the sample to the sequence\n",
        "                input_ids = torch.cat((input_ids, next_token.unsqueeze(0)), dim=1)\n",
        "\n",
        "                # If we hit the end-of-sequence token, stop early.\n",
        "                if self.tokenizer.eos_token_id and next_token.item() == self.tokenizer.eos_token_id:\n",
        "                    break\n",
        "\n",
        "        output_text = self.tokenizer.decode(input_ids.squeeze().tolist(), skip_special_tokens=True)\n",
        "        return output_text\n"
      ],
      "metadata": {
        "id": "tFRxcK8mnRAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = TopPTextGenerator(model, tokenizer, top_p=0.9, temperature=1.0, device=device)"
      ],
      "metadata": {
        "id": "hq2jVg-SnRET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"where is United States\"\n",
        "system=\"<|startheader|>system:<|endheader|>You are an AI assistant. You will be given a task. You must generate a detailed and long answer.<|endoftext|>\"\n",
        "input_text = f\"{system}<|startheader|>user:<|endheader|>{prompt}<|endoftext|><|startheader|>assistant:<|endheader|>\""
      ],
      "metadata": {
        "id": "fsvLoo0RnRIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = generator.generate(input_text, seq_len=20)\n",
        "print(\"Generated Text (Top-P):\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E43aljAfnaG0",
        "outputId": "afebad50-95f4-4c59-948a-17e137f4863c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text (Top-P):\n",
            "system:You are an AI assistant. You will be given a task. You must generate a detailed and long answer.user:where is United Statesassistant:null REST deepest Santorum Senatorooters CromeanrossFilm-----lovingParis zincBrother Abortion Medicoccdb cd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated = generate_text_k(model, tokenizer, input_text,device, max_length=10, k=10)\n",
        "print(\"Generated text (Top-K):\\n\", generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLyxnjIMnaMN",
        "outputId": "311e55b2-998f-4e65-90e8-af818fbe668f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text (Top-K):\n",
            " system:You are an AI assistant. You will be given a task. You must generate a detailed and long answer.user:where is United Statesassistant: MOT PhaseMarilantro programmers GP scholarlyfounded?375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated = generate_text(model, tokenizer, input_text,device, max_length=10)\n",
        "print(\"Generated text (Greedy):\\n\", generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZxjRaJcnaPm",
        "outputId": "4a0706c2-2c06-440d-becb-cd5e315f6042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text (Greedy):\n",
            " system:You are an AI assistant. You will be given a task. You must generate a detailed and long answer.user:where is United Statesassistant: trades anteriorpastbotricularacco- Assignment DimTokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c=0\n",
        "for i in hermes_instruct:\n",
        "    #print(i['input_ids'][1])\n",
        "    #print(i['labels'][1])\n",
        "    #text2= tokenizer.decode(i['input_ids'][0], skip_special_tokens=False)\n",
        "    #print(text2)\n",
        "    print(\"--------------------------------------------------\")\n",
        "    #text=tokenizer.decode(i['labels'][0], skip_special_tokens=False)\n",
        "    #print(text)\n",
        "    #print(f\"\\r{c++}\",end=\"\")\n",
        "    print(i[\"text\"])\n",
        "    break"
      ],
      "metadata": {
        "id": "44fHc06vnaSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer.encode(input_text),skip_special_tokens=False)"
      ],
      "metadata": {
        "id": "DidnEScnno8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(50258)"
      ],
      "metadata": {
        "id": "-U3U-M7mnpBQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}