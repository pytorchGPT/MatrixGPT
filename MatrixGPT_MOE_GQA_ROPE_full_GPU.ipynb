{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# developer Mujtaba Ghulami for learn multihead attention model and sample PositionalEncoding"
      ],
      "metadata": {
        "id": "9kQah6Fq7bvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3bXG9s21PmT"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets transformers\n",
        "!pip install torchinfo\n",
        "!pip install torchviz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "#from torchviz import make_dot\n",
        "#from torchinfo import summary\n",
        "from transformers import GPT2Tokenizer\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import pandas as pd\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "_NmFVZa41bFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def apply_rope(x, rope_freqs):\n",
        "    # x: (batch, seq_len, num_heads, head_dim)\n",
        "    bsz, seq_len, num_heads, head_dim = x.shape\n",
        "    x_ = x.view(bsz, seq_len, num_heads, head_dim // 2, 2)\n",
        "    cos, sin = rope_freqs\n",
        "    cos = cos[:seq_len, None, None, :, :]\n",
        "    sin = sin[:seq_len, None, None, :, :]\n",
        "    x_out = torch.cat([\n",
        "        x_[..., 0] * cos - x_[..., 1] * sin,\n",
        "        x_[..., 0] * sin + x_[..., 1] * cos\n",
        "    ], dim=-1)\n",
        "    return x_out.view(bsz, seq_len, num_heads, head_dim)\n",
        "\n",
        "def build_rope_cache(max_seq_len, head_dim, base=10000):\n",
        "    freqs = 1.0 / (base ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
        "    positions = torch.arange(max_seq_len).float()\n",
        "    angles = torch.einsum('i,j->ij', positions, freqs)  # (seq_len, head_dim/2)\n",
        "    cos = torch.cos(angles).unsqueeze(-1)\n",
        "    sin = torch.sin(angles).unsqueeze(-1)\n",
        "    return cos, sin\n",
        "\n",
        "class GQAAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_q_heads, num_kv_heads, max_len=2048, rope=True):\n",
        "        super().__init__()\n",
        "        self.num_q_heads = num_q_heads\n",
        "        self.num_kv_heads = num_kv_heads\n",
        "        self.head_dim = embed_dim // num_q_heads\n",
        "        self.rope = rope\n",
        "\n",
        "        self.q_proj = nn.Linear(embed_dim, num_q_heads * self.head_dim)\n",
        "        self.k_proj = nn.Linear(embed_dim, num_kv_heads * self.head_dim)\n",
        "        self.v_proj = nn.Linear(embed_dim, num_kv_heads * self.head_dim)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        if rope:\n",
        "            self.register_buffer(\"rope_freqs\", build_rope_cache(max_len, self.head_dim), persistent=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        bsz, seq_len, _ = x.size()\n",
        "\n",
        "        q = self.q_proj(x).view(bsz, seq_len, self.num_q_heads, self.head_dim)\n",
        "        k = self.k_proj(x).view(bsz, seq_len, self.num_kv_heads, self.head_dim)\n",
        "        v = self.v_proj(x).view(bsz, seq_len, self.num_kv_heads, self.head_dim)\n",
        "\n",
        "        # Expand K/V to match Q heads (GQA)\n",
        "        if self.num_q_heads != self.num_kv_heads:\n",
        "            k = k.repeat_interleave(self.num_q_heads // self.num_kv_heads, dim=2)\n",
        "            v = v.repeat_interleave(self.num_q_heads // self.num_kv_heads, dim=2)\n",
        "\n",
        "        # Apply RoPE\n",
        "        if self.rope:\n",
        "            q = apply_rope(q, self.rope_freqs)\n",
        "            k = apply_rope(k, self.rope_freqs)\n",
        "\n",
        "        # Attention\n",
        "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim, device=x.device, dtype=q.dtype))\n",
        "        #attn_weights = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), 1).bool()\n",
        "        attn_weights = attn_weights.masked_fill(causal_mask, float('-inf'))\n",
        "        attn_probs = torch.softmax(attn_weights, dim=-1)\n",
        "\n",
        "        attn_output = torch.matmul(attn_probs, v)\n",
        "        attn_output = attn_output.view(bsz, seq_len, -1)\n",
        "\n",
        "        return self.out_proj(attn_output)\n",
        "\n",
        "class MOE(nn.Module):\n",
        "    def __init__(self, embed_dim, ff_dim, num_experts=4, top_k=1):\n",
        "        super(MOE, self).__init__()\n",
        "        self.num_experts = num_experts\n",
        "        self.top_k = top_k\n",
        "\n",
        "        # Create experts (each is a feed-forward network)\n",
        "        self.experts = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(embed_dim, ff_dim),\n",
        "                nn.SiLU(),\n",
        "                nn.Linear(ff_dim, embed_dim)\n",
        "            )\n",
        "            for _ in range(num_experts)\n",
        "        ])\n",
        "\n",
        "        # Gating network\n",
        "        self.gate = nn.Linear(embed_dim, num_experts)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, embed_dim)\n",
        "        gate_logits = self.gate(x)  # (batch, seq_len, num_experts)\n",
        "        gate_scores = torch.softmax(gate_logits, dim=-1)  # probabilities\n",
        "\n",
        "        # Get top-k experts per token\n",
        "        topk_scores, topk_indices = torch.topk(gate_scores, self.top_k, dim=-1)  # (batch, seq_len, top_k)\n",
        "\n",
        "        # Initialize combined output\n",
        "        output = torch.zeros_like(x)\n",
        "\n",
        "        for i in range(self.top_k):\n",
        "            expert_idx = topk_indices[..., i]  # (batch, seq_len)\n",
        "            score = topk_scores[..., i].unsqueeze(-1)  # (batch, seq_len, 1)\n",
        "\n",
        "            # Mask tokens for this expert\n",
        "            for exp_id, expert in enumerate(self.experts):\n",
        "                mask = (expert_idx == exp_id).unsqueeze(-1)  # (batch, seq_len, 1)\n",
        "                if mask.any():\n",
        "                    exp_out = expert(x * mask)  # Apply expert only where mask is True\n",
        "                    output += exp_out * score * mask\n",
        "\n",
        "        return output\n",
        "\n",
        "# Causal Self-Attention\n",
        "class MatrixModel(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim,num_experts):\n",
        "        super(MatrixModel, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # GQA Attention\n",
        "        self.gqa = GQAAttention(embed_dim, num_q_heads=num_heads, num_kv_heads=num_heads//2, max_len=2048, rope=True)\n",
        "        # Feed-forward network following attention\n",
        "        self.ffn = MOE(embed_dim, ff_dim, num_experts=num_experts, top_k=1)\n",
        "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
        "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # inputs shape: (batch, seq_len, embed_dim)\n",
        "        attn_output = self.gqa(inputs)\n",
        "\n",
        "        # Residual connection and layer normalization\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        # Pass through feed-forward network\n",
        "        ffn_output = self.ffn(out1)\n",
        "\n",
        "        output = self.layernorm2(out1 + ffn_output)\n",
        "        return output\n",
        "\n",
        "\n",
        "# Full Brain-Inspired Model Module (now a Causal Language Model)\n",
        "class MatrixGPT_MOE_GQA_ROPE(nn.Module):\n",
        "    def __init__(self, vocab_size, max_length, embed_dim, num_layers,\n",
        "                 num_heads, key_dim, ff_dim,num_experts):\n",
        "        super(MatrixGPT_MOE_GQA_ROPE, self).__init__()\n",
        "        self.max_length = max_length\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        # Positional encoding layer\n",
        "\n",
        "\n",
        "        # List of Causal Self-Attention layers\n",
        "        self.MatrixModel_layers = nn.ModuleList([\n",
        "            MatrixModel(embed_dim, num_heads, ff_dim,num_experts)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.output_layer = nn.Linear(self.embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch, seq_len)\n",
        "        x = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
        "\n",
        "        for attn in self.MatrixModel_layers:\n",
        "            x= attn(x)\n",
        "\n",
        "        # Output layer; for language modeling, output logits over vocab for each token.\n",
        "        logits = self.output_layer(x)\n",
        "        # For classification we often apply softmax externally (e.g., in loss function)\n",
        "        return logits\n",
        "\n"
      ],
      "metadata": {
        "id": "E4pHhwOQ1bPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50259\n",
        "max_length = 1024\n",
        "embed_dim = 1536\n",
        "num_layers = 16   # Increase depth for better representation\n",
        "num_heads = 8\n",
        "key_dim = 192  # Should be embed_dim // num_heads\n",
        "ff_dim = 6144\n",
        "num_experts=4\n",
        "model_path= \"/kaggle/working/MatrixGPT.pth\" #\"/kaggle/input/matrix/MatrixGPT.pth\" #\"/content/drive/MyDrive/brain_p/MatrixGPT.pth\" # \"/kaggle/working/MatrixGPT.pth\"\n",
        "save_path = \"/kaggle/working/MatrixGPT.pth\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "IoaLEKt71bSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "model = MatrixGPT_MOE_GQA_ROPE(\n",
        "    vocab_size=vocab_size,\n",
        "    max_length=max_length,\n",
        "    embed_dim=embed_dim,\n",
        "    num_layers=num_layers,\n",
        "    num_heads=num_heads,\n",
        "    key_dim=key_dim,\n",
        "    ff_dim=ff_dim,\n",
        "    num_experts=num_experts\n",
        ")"
      ],
      "metadata": {
        "id": "6XXgfvay2vUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "tDMLq3s2_VIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "wDu0mG4q2ve0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Define header tokens\n",
        "START_HEADER = \"<|startheader|>\"\n",
        "END_HEADER = \"<|endheader|>\"\n",
        "\n",
        "# Load the dataset\n",
        "\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "# Set up padding and end-of-text token\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Use EOS token as padding\n",
        "\n",
        "# Add special tokens: ensure EOS and header tokens are added\n",
        "special_tokens_dict = {\n",
        "    \"eos_token\": \"<|endoftext|>\",\n",
        "    \"additional_special_tokens\": [START_HEADER, END_HEADER]\n",
        "}\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "def _return_header(message) -> str:\n",
        "    role = message.get(\"from\", \"\")\n",
        "    if role == \"system\":\n",
        "        return \"system:\"\n",
        "    elif role == \"gpt\":\n",
        "        return \"assistant:\"\n",
        "    elif role == \"human\":\n",
        "        return \"user:\"\n",
        "    return \"unknown:\"\n",
        "\n",
        "def encode_header(message):\n",
        "    header = _return_header(message)\n",
        "    # Wrap the header text with start and end header tokens\n",
        "    return f\"{START_HEADER}{header}{END_HEADER}\"\n",
        "\n",
        "def encode_message(message) -> str:\n",
        "    text = encode_header(message)\n",
        "    text += message[\"value\"].strip()\n",
        "    text += \"<|endoftext|>\"  # Append the correct end-of-text token\n",
        "    return text\n",
        "\n",
        "def encode_dialog_prompt(dialog):\n",
        "    # Concatenate all messages in the dialog into one string.\n",
        "    return \"\".join(encode_message(message) for message in dialog)\n",
        "\n",
        "def hermes_ins(batch):\n",
        "    # Encode the conversation in each batch item\n",
        "    texts = [encode_dialog_prompt(item['conversations']) for item in batch]\n",
        "    tokenized = tokenizer(\n",
        "        texts,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,  # You may change padding behavior if desired\n",
        "        truncation=True,\n",
        "        max_length=max_length + 1  # Increased max_length by 1 to account for labels\n",
        "    )\n",
        "    input_ids = tokenized[\"input_ids\"].long()\n",
        "    inputs = input_ids[:, :-1]\n",
        "    labels = input_ids[:, 1:]\n",
        "    return {\"input_ids\": inputs, \"labels\": labels, \"text\": texts}\n",
        "\n",
        "# Create DataLoader\n"
      ],
      "metadata": {
        "id": "G5f3xbZQ1bXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#!wget https://huggingface.co/datasets/teknium/OpenHermes-2.5/resolve/main/openhermes2_5.json"
      ],
      "metadata": {
        "id": "GKmvP9AT_Co7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the JSON data using pandas\n",
        "df = pd.read_json(\"openhermes2_5.json\")\n",
        "\n",
        "# Create a Dataset from the pandas DataFrame\n",
        "OpenHermes = Dataset.from_pandas(df)"
      ],
      "metadata": {
        "id": "fSA848BV9ZNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OpenHermes = load_dataset(\"teknium/OpenHermes-2.5\", split='train', trust_remote_code=True)\n",
        "hermes_instruct = DataLoader(OpenHermes, batch_size=1, shuffle=True, collate_fn=hermes_ins)"
      ],
      "metadata": {
        "id": "wAeyZLpmBtvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_all(model,optimizer,loss):\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,  }\n",
        "    torch.save(checkpoint, save_path)\n",
        "    time.sleep(7)"
      ],
      "metadata": {
        "id": "LYGxU-Dm9GwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "checkpoint = torch.load(model_path, map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "loss = checkpoint['loss']\n",
        "del checkpoint"
      ],
      "metadata": {
        "id": "G3dt0qgh1baJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "str(loss.item())"
      ],
      "metadata": {
        "id": "m0XyfzoEjFPS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21adac6e-780d-4891-9c54-f69103378352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.4338739812374115'"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save = 1  # Initialize save counter\n",
        "saved=0\n",
        "epoch=0\n",
        "model.train()\n",
        "\n",
        "for epoch in range(1):  # Change to desired number of epochs\n",
        "    epoch += 1\n",
        "    for batch in hermes_instruct:\n",
        "        inputs = batch[\"input_ids\"].to(device)\n",
        "        targets = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"\\rLoss: {str(loss.item())[:5]} epoch: {epoch} saved: {saved} cycle: {save}\", end=\"\\t\\t\\t\")\n",
        "        save = save+1\n",
        "        if save > 3500:\n",
        "            save = 0\n",
        "            save_all(model, optimizer, loss)  # Save model\n",
        "            saved =saved+1\n",
        "            if saved % 3 == 0:\n",
        "                #upload()\n",
        "                pass\n",
        "            break\n",
        "            print(f\"\\rModel saved\", end=\"\")\n"
      ],
      "metadata": {
        "id": "-0PrhTs41bdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize parameters\n",
        "save = 1  # Initialize save counter\n",
        "saved = 0\n",
        "epoch = 0\n",
        "model.train()\n",
        "\n",
        "# Gradient accumulation settings\n",
        "accum_steps = 16  # Number of small batches to accumulate gradients over\n",
        "effective_batch_size = 1 * accum_steps  # Simulated larger batch size\n",
        "#print(f\"Simulating effective batch size: {effective_batch_size}\")\n",
        "\n",
        "for epoch in range(1):  # Change to desired number of epochs\n",
        "    epoch += 1\n",
        "    optimizer.zero_grad()  # Clear gradients at the start of each epoch\n",
        "    for batch_idx, batch in enumerate(hermes_instruct):\n",
        "        inputs = batch[\"input_ids\"].to(device)\n",
        "        targets = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "\n",
        "        # Scale loss to account for gradient accumulation\n",
        "        loss = loss / accum_steps\n",
        "        loss.backward()  # Accumulate gradients\n",
        "\n",
        "        # Perform optimization step after accum_steps batches\n",
        "        if (batch_idx + 1) % accum_steps == 0:\n",
        "            optimizer.step()  # Update model parameters\n",
        "            optimizer.zero_grad()  # Clear gradients after update\n",
        "            print(f\"\\rLoss: {str(loss.item() * accum_steps)[:5]} epoch: {epoch} saved: {saved} cycle: {save}\", end=\"\\t\\t\\t\")\n",
        "\n",
        "        save += 1\n",
        "        if save > 3500:\n",
        "            save = 0\n",
        "            save_all(model, optimizer,str(loss.item() * accum_steps)[:5])  # Save model\n",
        "            saved += 1\n",
        "            if saved % 3 == 0:\n",
        "                # upload()\n",
        "                pass\n",
        "            #break\n",
        "            print(f\"\\rModel saved\", end=\"\")"
      ],
      "metadata": {
        "id": "bG6XhmVl05-i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "794e1c9a-dce4-4284-ea11-ba5280879d93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 6.279 epoch: 1 saved: 3 cycle: 937\t\t\t"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_sampling(logits, k):\n",
        "    \"\"\"\n",
        "    Select the next token using top-k sampling.\n",
        "    Args:\n",
        "        logits (Tensor): Logits for the current token with shape [vocab_size].\n",
        "        k (int): The number of top tokens to sample from.\n",
        "    Returns:\n",
        "        int: The token id sampled from the top-k distribution.\n",
        "    \"\"\"\n",
        "    # Apply softmax to get probabilities.\n",
        "    probabilities = F.softmax(logits, dim=-1)\n",
        "    # Get the top-k token ids and their probabilities.\n",
        "    topk_probs, topk_indices = torch.topk(probabilities, k)\n",
        "    # Normalize the top-k probabilities.\n",
        "    topk_probs = topk_probs / torch.sum(topk_probs)\n",
        "    # Sample one token id from the top-k distribution.\n",
        "    next_token_id = torch.multinomial(topk_probs, 1).item()\n",
        "    # Get the corresponding token id from topk_indices.\n",
        "    return topk_indices[next_token_id].item()\n",
        "\n",
        "def generate_text_k(model, tokenizer, input_text,device, max_length=50, k=10):\n",
        "    model.eval()\n",
        "    # Tokenize the input text.\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "    generated = input_ids.tolist()[0]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            # Use only the last token as input along with the previous hidden state.\n",
        "            input_token = torch.tensor([[generated[-1]]]).to(device)\n",
        "            logits = model(input_token)\n",
        "            # Get logits for the last token (shape: [1, 1, vocab_size]) and remove unneeded dimensions.\n",
        "            next_token_logits = logits[:, -1, :].squeeze(0) # Corrected indexing\n",
        "\n",
        "            # Sample the next token using top-k sampling.\n",
        "            next_token_id = top_k_sampling(next_token_logits, k)\n",
        "            generated.append(next_token_id)\n",
        "\n",
        "            # Optionally, stop generation if the end-of-sequence token is generated.\n",
        "            if tokenizer.eos_token_id is not None and next_token_id == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    # Decode the complete generated token list.\n",
        "    generated_text = tokenizer.decode(generated, skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "def generate_text(model, tokenizer, input_text,device, max_length=50):\n",
        "    model.eval()\n",
        "    # Tokenize the input text.\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "    generated = input_ids.tolist()[0]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            # Provide the complete sequence each time to help the model consider context.\n",
        "            input_ids_tensor = torch.tensor([generated]).to(device)\n",
        "\n",
        "            # Get probability distribution for the next token.\n",
        "            logits = model(input_ids_tensor)\n",
        "            last_token_logits = logits[:, -1, :]  # shape (batch, vocab_size) # Corrected indexing\n",
        "            probabilities = torch.softmax(last_token_logits, dim=-1)\n",
        "            # Greedy sampling: choose the token with the highest probability.\n",
        "            next_token_id = torch.argmax(probabilities, dim=-1).item()\n",
        "            generated.append(next_token_id)\n",
        "\n",
        "            # Stop generation if the EOS token is produced.\n",
        "            if next_token_id == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    generated_text = tokenizer.decode(generated, skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "class TopPTextGenerator:\n",
        "    \"\"\"\n",
        "    A class to perform text generation using nucleus (top-p) sampling.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, tokenizer, top_p=0.9, temperature=1.0, device=None):\n",
        "        \"\"\"\n",
        "        model: PyTorch module that returns logits of shape [batch_size, seq_length, vocab_size]\n",
        "        tokenizer: A tokenizer with encode/decode methods and an eos_token_id attribute.\n",
        "        top_p: The cumulative probability threshold for nucleus sampling.\n",
        "        temperature: A factor to control randomness; higher values increase randomness.\n",
        "        device: torch.device to use.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.top_p = top_p\n",
        "        self.temperature = temperature\n",
        "        self.device = device\n",
        "\n",
        "    def nucleus_sampling(self, logits):\n",
        "        \"\"\"\n",
        "        Applies nucleus (top-p) filtering to the logits.\n",
        "        logits: Tensor of shape [vocab_size] representing logits for the next token.\n",
        "        Returns the logits with values filtered out that do not belong to the top-p cumulative distribution.\n",
        "        \"\"\"\n",
        "        # Apply temperature scaling\n",
        "        logits = logits / self.temperature\n",
        "\n",
        "        # Compute probabilities from logits\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # Sort the probabilities in descending order\n",
        "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "\n",
        "        # Compute cumulative probabilities of the sorted tensor\n",
        "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "        # Create a mask to filter out tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probs > self.top_p\n",
        "\n",
        "        # Shift the mask one token to the right to keep the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        # Prepare an output copy of logits to modify\n",
        "        filtered_logits = logits.clone()\n",
        "        # Get the indices to remove from the sorted token indices\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "        filtered_logits[indices_to_remove] = -float('Inf')\n",
        "        return filtered_logits\n",
        "\n",
        "    def generate(self, prompt, seq_len=50):\n",
        "        \"\"\"\n",
        "        Generates text conditioned on a prompt.\n",
        "\n",
        "        prompt: Starting text string.\n",
        "        seq_len: Maximum number of tokens to generate.\n",
        "        Returns the generated text string.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        # Encode the prompt using the GPT-2 tokenizer.\n",
        "        token_ids = self.tokenizer.encode(prompt)\n",
        "        input_ids = torch.tensor(token_ids, dtype=torch.long, device=self.device).unsqueeze(0)  # shape: [1, seq_length]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(seq_len):\n",
        "                # Get logits from the model (assuming model returns logits for every token)\n",
        "                logits = self.model(input_ids)  # shape: [1, current_seq_len, vocab_size]\n",
        "                next_token_logits = logits[0, -1, :]  # shape: [vocab_size] # Corrected indexing\n",
        "\n",
        "                # Apply nucleus sampling filtering to logits\n",
        "                filtered_logits = self.nucleus_sampling(next_token_logits)\n",
        "\n",
        "                # Convert filtered logits to probabilities and sample the next token\n",
        "                probs = F.softmax(filtered_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "                # Append the sample to the sequence\n",
        "                input_ids = torch.cat((input_ids, next_token.unsqueeze(0)), dim=1)\n",
        "\n",
        "                # If we hit the end-of-sequence token, stop early.\n",
        "                if self.tokenizer.eos_token_id and next_token.item() == self.tokenizer.eos_token_id:\n",
        "                    break\n",
        "\n",
        "        output_text = self.tokenizer.decode(input_ids.squeeze().tolist(), skip_special_tokens=True)\n",
        "        return output_text\n"
      ],
      "metadata": {
        "id": "oQjEDIXT1bit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = TopPTextGenerator(model, tokenizer, top_p=0.9, temperature=1.0, device=device)"
      ],
      "metadata": {
        "id": "lJ5TtDVF1bl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"where is United States\"\n",
        "system=\"<|startheader|>system:<|endheader|>You are an AI assistant. You will be given a task. You must generate a detailed and long answer.<|endoftext|>\"\n",
        "input_text = f\"{system}<|startheader|>user:<|endheader|>{prompt}<|endoftext|><|startheader|>assistant:<|endheader|>\""
      ],
      "metadata": {
        "id": "Kq2gQ1hO7BtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = generator.generate(input_text, seq_len=20)\n",
        "print(\"Generated Text (Top-P):\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "W6CmgLBm7B4h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5e7d50b-68c6-48ec-e72a-e3f3cbe1584f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text (Top-P):\n",
            "system:You are an AI assistant. You will be given a task. You must generate a detailed and long answer.user:where is United Statesassistant:useruserusersystemuserusersystemsystemsystemusersystemsystemuserusersystemuseruseruserusersystem\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated = generate_text_k(model, tokenizer, input_text,device, max_length=10, k=10)\n",
        "print(\"Generated text (Top-K):\\n\", generated)"
      ],
      "metadata": {
        "id": "Bi05OQmp7B1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03450005-0473-4770-f641-42882798ca00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text (Top-K):\n",
            " system:You are an AI assistant. You will be given a task. You must generate a detailed and long answer.user:where is United Statesassistant:usersystemuseruseruseruserusersystemsystemuser\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated = generate_text(model, tokenizer, input_text,device, max_length=10)\n",
        "print(\"Generated text (Greedy):\\n\", generated)"
      ],
      "metadata": {
        "id": "a3ia4md07B8L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8af23e72-8af1-4660-fab3-cc91e93e038d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text (Greedy):\n",
            " system:You are an AI assistant. You will be given a task. You must generate a detailed and long answer.user:where is United Statesassistant:useruseruseruseruseruseruseruseruseruser\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c=0\n",
        "for i in hermes_instruct:\n",
        "    #print(i['input_ids'][1])\n",
        "    #print(i['labels'][1])\n",
        "    #text2= tokenizer.decode(i['input_ids'][0], skip_special_tokens=False)\n",
        "    #print(text2)\n",
        "    print(\"--------------------------------------------------\")\n",
        "    #text=tokenizer.decode(i['labels'][0], skip_special_tokens=False)\n",
        "    #print(text)\n",
        "    #print(f\"\\r{c++}\",end=\"\")\n",
        "    print(i[\"text\"])\n",
        "    break"
      ],
      "metadata": {
        "id": "UhYqORflCB-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd359ad8-a3c6-4466-891c-05efe000a51e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "['<|startheader|>system:<|endheader|>You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.<|endoftext|><|startheader|>user:<|endheader|>Article: Communication is observed within the plant organism, i.e. within plant cells and between plant cells, between plants of the same or related species, and between plants and non-plant organisms, especially in the root zone. Plant roots communicate with rhizome bacteria, fungi, and insects within the soil. These interactions are governed by syntactic, pragmatic, and semantic rules,[citation needed] and are possible because of the decentralized \"nervous system\" of plants. The original meaning of the word \"neuron\" in Greek is \"vegetable fiber\" and recent research has shown that most of the microorganism plant communication processes are neuron-like. Plants also communicate via volatiles when exposed to herbivory attack behavior, thus warning neighboring plants. In parallel they produce other volatiles to attract parasites which attack these herbivores. In stress situations plants can overwrite the genomes they inherited from their parents and revert to that of their grand- or great-grandparents.[citation needed]\\n\\nQuestion: What is the meaning of the word neuron in Greek?<|endoftext|><|startheader|>assistant:<|endheader|>The meaning of the word \"neuron\" in Greek is \"vegetable fiber.\"<|endoftext|>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer.encode(input_text),skip_special_tokens=False)"
      ],
      "metadata": {
        "id": "NeA1UINJDGTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(50258)"
      ],
      "metadata": {
        "id": "8SkS18_tQg8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-wgDfYptegUG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}