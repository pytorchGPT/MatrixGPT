{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# developer Mujtaba Ghulami for learn multihead attention model and sample PositionalEncoding"
      ],
      "metadata": {
        "id": "38e_ygHC3qfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3bXG9s21PmT"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets transformers\n",
        "!pip install torchinfo\n",
        "!pip install torchviz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "#from torchviz import make_dot\n",
        "#from torchinfo import summary\n",
        "from transformers import GPT2Tokenizer\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import pandas as pd\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "_NmFVZa41bFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional Encoding Module\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, max_len, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        # Create constant 'pe' matrix with values dependent on position and dimension.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # even indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # odd indices\n",
        "        pe = pe.unsqueeze(0)  # Add batch dimension\n",
        "        # Register pe as a buffer so it is part of the module state but not a parameter.\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch, seq_len, d_model)\n",
        "        seq_len = x.size(1)\n",
        "        x = x + self.pe[:, :seq_len, :]\n",
        "        return x\n",
        "\n",
        "# Causal Self-Attention\n",
        "class MatrixModel(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
        "        super(MatrixModel, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.mha = nn.MultiheadAttention(embed_dim, num_heads,batch_first=True)\n",
        "\n",
        "        # Feed-forward network following attention\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.SiLU(), #nn.GELU(),\n",
        "            nn.Linear(ff_dim, embed_dim)\n",
        "        )\n",
        "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
        "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # inputs shape: (batch, seq_len, embed_dim)\n",
        "        attn_output, _ = self.mha(inputs, inputs, inputs, attn_mask=self._generate_causal_mask(inputs.size(1)).to(inputs.device))\n",
        "\n",
        "        # Residual connection and layer normalization\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        # Pass through feed-forward network\n",
        "        ffn_output = self.ffn(out1)\n",
        "\n",
        "        output = self.layernorm2(out1 + ffn_output)\n",
        "        return output\n",
        "\n",
        "    def _generate_causal_mask(self, seq_len):\n",
        "        return torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
        "\n",
        "\n",
        "\n",
        "# Full Brain-Inspired Model Module (now a Causal Language Model)\n",
        "class MatrixGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, max_length, embed_dim, num_layers,\n",
        "                 num_heads, ff_dim):\n",
        "        super(MatrixGPT, self).__init__()\n",
        "        self.max_length = max_length\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        # Positional encoding layer\n",
        "        self.positional_encoding = PositionalEncoding(max_length, embed_dim)\n",
        "\n",
        "        # List of Causal Self-Attention layers\n",
        "        self.MatrixModel_layers = nn.ModuleList([\n",
        "            MatrixModel(embed_dim, num_heads, ff_dim)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.output_layer = nn.Linear(self.embed_dim, vocab_size)\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch, seq_len)\n",
        "        x = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
        "        x= self.positional_encoding(x)\n",
        "\n",
        "        for attn in self.MatrixModel_layers:\n",
        "            x= attn(x)\n",
        "\n",
        "        # Output layer; for language modeling, output logits over vocab for each token.\n",
        "        logits = self.output_layer(x)\n",
        "        # For classification we often apply softmax externally (e.g., in loss function)\n",
        "        return logits\n",
        "\n"
      ],
      "metadata": {
        "id": "E4pHhwOQ1bPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50259\n",
        "max_length = 1024\n",
        "embed_dim = 1536\n",
        "num_layers = 16   # Increase depth for better representation\n",
        "num_heads = 8\n",
        "ff_dim = 6144\n",
        "model_path= \"/kaggle/working/MatrixGPT.pth\" #\"/kaggle/input/matrix/MatrixGPT.pth\" #\"/content/drive/MyDrive/brain_p/MatrixGPT.pth\" # \"/kaggle/working/MatrixGPT.pth\"\n",
        "save_path = \"/kaggle/working/MatrixGPT.pth\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "IoaLEKt71bSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "model = MatrixGPT(\n",
        "    vocab_size=vocab_size,\n",
        "    max_length=max_length,\n",
        "    embed_dim=embed_dim,\n",
        "    num_layers=num_layers,\n",
        "    num_heads=num_heads,\n",
        "    ff_dim=ff_dim,\n",
        ")"
      ],
      "metadata": {
        "id": "6XXgfvay2vUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "if torch.cuda.device_count() > 1:\n",
        "    #print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    #model = nn.DataParallel(model)\n",
        "    pass\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "tDMLq3s2_VIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # More efficient for multi-GPU\n",
        "# from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "# # Setup (more complex, but worth it)\n",
        "# torch.distributed.init_process_group(backend='nccl')\n",
        "# local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
        "# model = model.to(local_rank)\n",
        "# model = DDP(model, device_ids=[local_rank])\n",
        "# if isinstance(model, nn.DataParallel):\n",
        "#     original_model = model.module\n",
        "# else:\n",
        "#     original_model = model\n",
        "\n",
        "# # Save\n",
        "# torch.save(model.module.state_dict() if isinstance(model, nn.DataParallel)\n",
        "#            else model.state_dict(), \"checkpoint.pth\")"
      ],
      "metadata": {
        "id": "7VzaZg39kL0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Define header tokens\n",
        "START_HEADER = \"<|startheader|>\"\n",
        "END_HEADER = \"<|endheader|>\"\n",
        "\n",
        "# Load the dataset\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "# Set up padding and end-of-text token\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Use EOS token as padding\n",
        "\n",
        "# Add special tokens: ensure EOS and header tokens are added\n",
        "special_tokens_dict = {\n",
        "    \"eos_token\": \"<|endoftext|>\",\n",
        "    \"additional_special_tokens\": [START_HEADER, END_HEADER]\n",
        "}\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "def _return_header(message) -> str:\n",
        "    role = message.get(\"from\", \"\")\n",
        "    if role == \"system\":\n",
        "        return \"system:\"\n",
        "    elif role == \"gpt\":\n",
        "        return \"assistant:\"\n",
        "    elif role == \"human\":\n",
        "        return \"user:\"\n",
        "    return \"unknown:\"\n",
        "\n",
        "def encode_header(message):\n",
        "    header = _return_header(message)\n",
        "    # Wrap the header text with start and end header tokens\n",
        "    return f\"{START_HEADER}{header}{END_HEADER}\"\n",
        "\n",
        "def encode_message(message) -> str:\n",
        "    text = encode_header(message)\n",
        "    text += message[\"value\"].strip()\n",
        "    text += \"<|endoftext|>\"  # Append the correct end-of-text token\n",
        "    return text\n",
        "\n",
        "def encode_dialog_prompt(dialog):\n",
        "    # Concatenate all messages in the dialog into one string.\n",
        "    return \"\".join(encode_message(message) for message in dialog)\n",
        "\n",
        "def hermes_ins(batch):\n",
        "    # Encode the conversation in each batch item\n",
        "    texts = [encode_dialog_prompt(item['conversations']) for item in batch]\n",
        "    tokenized = tokenizer(\n",
        "        texts,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,  # You may change padding behavior if desired\n",
        "        truncation=True,\n",
        "        max_length=max_length + 1  # Increased max_length by 1 to account for labels\n",
        "    )\n",
        "    input_ids = tokenized[\"input_ids\"].long()\n",
        "    inputs = input_ids[:, :-1]\n",
        "    labels = input_ids[:, 1:]\n",
        "    return {\"input_ids\": inputs, \"labels\": labels, \"text\": texts}\n",
        "\n",
        "# Create DataLoader\n"
      ],
      "metadata": {
        "id": "G5f3xbZQ1bXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#!wget https://huggingface.co/datasets/teknium/OpenHermes-2.5/resolve/main/openhermes2_5.json"
      ],
      "metadata": {
        "id": "GKmvP9AT_Co7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the JSON data using pandas\n",
        "#df = pd.read_json(\"openhermes2_5.json\")\n",
        "\n",
        "# Create a Dataset from the pandas DataFrame\n",
        "#OpenHermes = Dataset.from_pandas(df)"
      ],
      "metadata": {
        "id": "fSA848BV9ZNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OpenHermes = load_dataset(\"teknium/OpenHermes-2.5\", split='train')\n",
        "hermes_instruct = DataLoader(OpenHermes, batch_size=1, shuffle=True, collate_fn=hermes_ins)"
      ],
      "metadata": {
        "id": "wAeyZLpmBtvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_all(model, optimizer, loss, save_path):\n",
        "    # Handle wrapped models (DDP/DataParallel)\n",
        "    model_state = model.module.state_dict() if hasattr(model, 'module') else model.state_dict()\n",
        "\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model_state,\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "    }\n",
        "    torch.save(checkpoint, save_path)\n",
        "    time.sleep(7)"
      ],
      "metadata": {
        "id": "LYGxU-Dm9GwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize once (don't re-initialize if already done)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)"
      ],
      "metadata": {
        "id": "jAxF0Iug453C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture\n",
        "# # Loading\n",
        "# checkpoint = torch.load(model_path, map_location=device)\n",
        "\n",
        "# # Handle wrapped models when loading\n",
        "# if hasattr(model, 'module'):\n",
        "#     model.module.load_state_dict(checkpoint['model_state_dict'])\n",
        "# else:\n",
        "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "# loss = checkpoint['loss']\n",
        "# del checkpoint"
      ],
      "metadata": {
        "id": "G3dt0qgh1baJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "str(loss.item())"
      ],
      "metadata": {
        "id": "m0XyfzoEjFPS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21adac6e-780d-4891-9c54-f69103378352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.4338739812374115'"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-0PrhTs41bdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_cosine_schedule_with_warmup\n",
        "import torch.nn.utils as nn_utils\n",
        "\n",
        "\n",
        "# Setup\n",
        "num_epochs = 10\n",
        "save_every = 3500\n",
        "total_steps = num_epochs * len(hermes_instruct)\n",
        "warmup_steps = int(0.1 * total_steps)\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "# Gradient accumulation settings\n",
        "accum_steps = 16\n",
        "effective_batch_size = 1 * accum_steps\n",
        "print(f\"Simulating effective batch size: {effective_batch_size}\")\n",
        "\n",
        "# Training loop\n",
        "global_step = 0\n",
        "saved = 0\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()  # Clear at start of epoch\n",
        "\n",
        "    for batch_idx, batch in enumerate(hermes_instruct):\n",
        "        inputs = batch[\"input_ids\"].to(device)\n",
        "        targets = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "\n",
        "        # Scale loss for gradient accumulation\n",
        "        loss = loss / accum_steps\n",
        "        loss.backward()  # Accumulate gradients\n",
        "\n",
        "        # Perform optimization step after accum_steps batches\n",
        "        if (batch_idx + 1) % accum_steps == 0:\n",
        "            # Clip gradients before optimizer step\n",
        "            nn_utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Display unscaled loss\n",
        "            actual_loss = loss.item() * accum_steps\n",
        "            current_lr = scheduler.get_last_lr()[0]\n",
        "            print(f\"\\rEpoch: {epoch+1}/{num_epochs} | \"\n",
        "                  f\"Batch: {batch_idx+1}/{len(hermes_instruct)} | \"\n",
        "                  f\"Loss: {actual_loss:.4f} | \"\n",
        "                  f\"LR: {current_lr:.2e} | \"\n",
        "                  f\"Saved: {saved}\", end=\"\")\n",
        "\n",
        "        # Increment global step counter\n",
        "        global_step += 1\n",
        "\n",
        "        # Save checkpoint every save_every steps\n",
        "        if global_step % save_every == 0:\n",
        "            # Apply any remaining accumulated gradients before saving\n",
        "            if (batch_idx + 1) % accum_steps != 0:\n",
        "                nn_utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            actual_loss = loss.item() * accum_steps\n",
        "            save_all(model, optimizer, f\"{actual_loss:.4f}\",save_path)\n",
        "            saved += 1\n",
        "            print(f\"\\n✓ Model saved at step {global_step}\")\n",
        "\n",
        "            if saved % 3 == 0:\n",
        "                # upload()  # Optional cloud backup\n",
        "                pass\n",
        "\n",
        "    # Handle remaining gradients at end of epoch\n",
        "    if (batch_idx + 1) % accum_steps != 0:\n",
        "        nn_utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1} completed\")"
      ],
      "metadata": {
        "id": "bG6XhmVl05-i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "794e1c9a-dce4-4284-ea11-ba5280879d93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 6.279 epoch: 1 saved: 3 cycle: 937\t\t\t"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AH_WhsZJR6FL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import get_cosine_schedule_with_warmup\n",
        "# import torch.nn.utils as nn_utils\n",
        "\n",
        "# # Setup\n",
        "# num_epochs = 10\n",
        "# save_every = 3500\n",
        "# total_steps = num_epochs * len(hermes_instruct)\n",
        "# warmup_steps = int(0.1 * total_steps)\n",
        "\n",
        "# scheduler = get_cosine_schedule_with_warmup(\n",
        "#     optimizer,\n",
        "#     num_warmup_steps=warmup_steps,\n",
        "#     num_training_steps=total_steps\n",
        "# )\n",
        "\n",
        "# # Training\n",
        "# global_step = 0\n",
        "# saved = 0\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     epoch_loss = 0\n",
        "\n",
        "#     for batch_idx, batch in enumerate(hermes_instruct):\n",
        "#         inputs = batch[\"input_ids\"].to(device)\n",
        "#         targets = batch[\"labels\"].to(device)\n",
        "\n",
        "#         # Forward pass\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(inputs)\n",
        "#         loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "\n",
        "#         # Backward pass\n",
        "#         loss.backward()\n",
        "#         nn_utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "#         optimizer.step()\n",
        "#         scheduler.step()\n",
        "\n",
        "#         # Track metrics\n",
        "#         global_step += 1\n",
        "#         epoch_loss += loss.item()\n",
        "\n",
        "#         # Display progress\n",
        "#         print(f\"\\rEpoch: {epoch+1}/{num_epochs} | \"\n",
        "#               f\"Batch: {batch_idx+1}/{len(hermes_instruct)} | \"\n",
        "#               f\"Loss: {loss.item():.4f} | \"\n",
        "#               f\"LR: {scheduler.get_last_lr()[0]:.2e} | \"\n",
        "#               f\"Saved: {saved}\", end=\"\")\n",
        "\n",
        "#         # Periodic saving\n",
        "#         if global_step % save_every == 0:\n",
        "#             save_all(model, optimizer, loss,save_path)\n",
        "#             saved += 1\n",
        "#             print(f\"\\n✓ Model saved at step {global_step}\")\n",
        "\n",
        "#             if saved % 3 == 0:\n",
        "#                 # upload()  # Optional cloud upload\n",
        "#                 pass\n",
        "\n",
        "#     # Epoch summary\n",
        "#     avg_loss = epoch_loss / len(hermes_instruct)\n",
        "#     print(f\"\\nEpoch {epoch+1} completed | Avg Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "fzPKFEDLWE4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_sampling(logits, k):\n",
        "    \"\"\"\n",
        "    Select the next token using top-k sampling.\n",
        "    Args:\n",
        "        logits (Tensor): Logits for the current token with shape [vocab_size].\n",
        "        k (int): The number of top tokens to sample from.\n",
        "    Returns:\n",
        "        int: The token id sampled from the top-k distribution.\n",
        "    \"\"\"\n",
        "    # Apply softmax to get probabilities.\n",
        "    probabilities = F.softmax(logits, dim=-1)\n",
        "    # Get the top-k token ids and their probabilities.\n",
        "    topk_probs, topk_indices = torch.topk(probabilities, k)\n",
        "    # Normalize the top-k probabilities.\n",
        "    topk_probs = topk_probs / torch.sum(topk_probs)\n",
        "    # Sample one token id from the top-k distribution.\n",
        "    next_token_id = torch.multinomial(topk_probs, 1).item()\n",
        "    # Get the corresponding token id from topk_indices.\n",
        "    return topk_indices[next_token_id].item()\n",
        "\n",
        "def generate_text_k(model, tokenizer, input_text,device, max_length=50, k=10):\n",
        "    model.eval()\n",
        "    # Tokenize the input text.\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "    generated = input_ids.tolist()[0]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            # Use only the last token as input along with the previous hidden state.\n",
        "            input_token = torch.tensor([[generated[-1]]]).to(device)\n",
        "            logits = model(input_token)\n",
        "            # Get logits for the last token (shape: [1, 1, vocab_size]) and remove unneeded dimensions.\n",
        "            next_token_logits = logits[:, -1, :].squeeze(0) # Corrected indexing\n",
        "\n",
        "            # Sample the next token using top-k sampling.\n",
        "            next_token_id = top_k_sampling(next_token_logits, k)\n",
        "            generated.append(next_token_id)\n",
        "\n",
        "            # Optionally, stop generation if the end-of-sequence token is generated.\n",
        "            if tokenizer.eos_token_id is not None and next_token_id == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    # Decode the complete generated token list.\n",
        "    generated_text = tokenizer.decode(generated, skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "def generate_text(model, tokenizer, input_text,device, max_length=50):\n",
        "    model.eval()\n",
        "    # Tokenize the input text.\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "    generated = input_ids.tolist()[0]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            # Provide the complete sequence each time to help the model consider context.\n",
        "            input_ids_tensor = torch.tensor([generated]).to(device)\n",
        "\n",
        "            # Get probability distribution for the next token.\n",
        "            logits = model(input_ids_tensor)\n",
        "            last_token_logits = logits[:, -1, :]  # shape (batch, vocab_size) # Corrected indexing\n",
        "            probabilities = torch.softmax(last_token_logits, dim=-1)\n",
        "            # Greedy sampling: choose the token with the highest probability.\n",
        "            next_token_id = torch.argmax(probabilities, dim=-1).item()\n",
        "            generated.append(next_token_id)\n",
        "\n",
        "            # Stop generation if the EOS token is produced.\n",
        "            if next_token_id == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    generated_text = tokenizer.decode(generated, skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "class TopPTextGenerator:\n",
        "    \"\"\"\n",
        "    A class to perform text generation using nucleus (top-p) sampling.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, tokenizer, top_p=0.9, temperature=1.0, device=None):\n",
        "        \"\"\"\n",
        "        model: PyTorch module that returns logits of shape [batch_size, seq_length, vocab_size]\n",
        "        tokenizer: A tokenizer with encode/decode methods and an eos_token_id attribute.\n",
        "        top_p: The cumulative probability threshold for nucleus sampling.\n",
        "        temperature: A factor to control randomness; higher values increase randomness.\n",
        "        device: torch.device to use.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.top_p = top_p\n",
        "        self.temperature = temperature\n",
        "        self.device = device\n",
        "\n",
        "    def nucleus_sampling(self, logits):\n",
        "        \"\"\"\n",
        "        Applies nucleus (top-p) filtering to the logits.\n",
        "        logits: Tensor of shape [vocab_size] representing logits for the next token.\n",
        "        Returns the logits with values filtered out that do not belong to the top-p cumulative distribution.\n",
        "        \"\"\"\n",
        "        # Apply temperature scaling\n",
        "        logits = logits / self.temperature\n",
        "\n",
        "        # Compute probabilities from logits\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # Sort the probabilities in descending order\n",
        "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "\n",
        "        # Compute cumulative probabilities of the sorted tensor\n",
        "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "        # Create a mask to filter out tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probs > self.top_p\n",
        "\n",
        "        # Shift the mask one token to the right to keep the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        # Prepare an output copy of logits to modify\n",
        "        filtered_logits = logits.clone()\n",
        "        # Get the indices to remove from the sorted token indices\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "        filtered_logits[indices_to_remove] = -float('Inf')\n",
        "        return filtered_logits\n",
        "\n",
        "    def generate(self, prompt, seq_len=50):\n",
        "        \"\"\"\n",
        "        Generates text conditioned on a prompt.\n",
        "\n",
        "        prompt: Starting text string.\n",
        "        seq_len: Maximum number of tokens to generate.\n",
        "        Returns the generated text string.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        # Encode the prompt using the GPT-2 tokenizer.\n",
        "        token_ids = self.tokenizer.encode(prompt)\n",
        "        input_ids = torch.tensor(token_ids, dtype=torch.long, device=self.device).unsqueeze(0)  # shape: [1, seq_length]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(seq_len):\n",
        "                # Get logits from the model (assuming model returns logits for every token)\n",
        "                logits = self.model(input_ids)  # shape: [1, current_seq_len, vocab_size]\n",
        "                next_token_logits = logits[0, -1, :]  # shape: [vocab_size] # Corrected indexing\n",
        "\n",
        "                # Apply nucleus sampling filtering to logits\n",
        "                filtered_logits = self.nucleus_sampling(next_token_logits)\n",
        "\n",
        "                # Convert filtered logits to probabilities and sample the next token\n",
        "                probs = F.softmax(filtered_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "                # Append the sample to the sequence\n",
        "                input_ids = torch.cat((input_ids, next_token.unsqueeze(0)), dim=1)\n",
        "\n",
        "                # If we hit the end-of-sequence token, stop early.\n",
        "                if self.tokenizer.eos_token_id and next_token.item() == self.tokenizer.eos_token_id:\n",
        "                    break\n",
        "\n",
        "        output_text = self.tokenizer.decode(input_ids.squeeze().tolist(), skip_special_tokens=True)\n",
        "        return output_text\n"
      ],
      "metadata": {
        "id": "oQjEDIXT1bit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = TopPTextGenerator(model, tokenizer, top_p=0.9, temperature=1.0, device=device)"
      ],
      "metadata": {
        "id": "lJ5TtDVF1bl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"where is United States\"\n",
        "system=\"<|startheader|>system:<|endheader|>You are an AI assistant. You will be given a task. You must generate a detailed and long answer.<|endoftext|>\"\n",
        "input_text = f\"{system}<|startheader|>user:<|endheader|>{prompt}<|endoftext|><|startheader|>assistant:<|endheader|>\""
      ],
      "metadata": {
        "id": "Kq2gQ1hO7BtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = generator.generate(input_text, seq_len=20)\n",
        "print(\"Generated Text (Top-P):\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "W6CmgLBm7B4h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5e7d50b-68c6-48ec-e72a-e3f3cbe1584f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text (Top-P):\n",
            "system:You are an AI assistant. You will be given a task. You must generate a detailed and long answer.user:where is United Statesassistant:useruserusersystemuserusersystemsystemsystemusersystemsystemuserusersystemuseruseruserusersystem\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated = generate_text_k(model, tokenizer, input_text,device, max_length=10, k=10)\n",
        "print(\"Generated text (Top-K):\\n\", generated)"
      ],
      "metadata": {
        "id": "Bi05OQmp7B1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03450005-0473-4770-f641-42882798ca00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text (Top-K):\n",
            " system:You are an AI assistant. You will be given a task. You must generate a detailed and long answer.user:where is United Statesassistant:usersystemuseruseruseruserusersystemsystemuser\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated = generate_text(model, tokenizer, input_text,device, max_length=10)\n",
        "print(\"Generated text (Greedy):\\n\", generated)"
      ],
      "metadata": {
        "id": "a3ia4md07B8L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8af23e72-8af1-4660-fab3-cc91e93e038d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text (Greedy):\n",
            " system:You are an AI assistant. You will be given a task. You must generate a detailed and long answer.user:where is United Statesassistant:useruseruseruseruseruseruseruseruseruser\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c=0\n",
        "for i in hermes_instruct:\n",
        "    #print(i['input_ids'][1])\n",
        "    #print(i['labels'][1])\n",
        "    #text2= tokenizer.decode(i['input_ids'][0], skip_special_tokens=False)\n",
        "    #print(text2)\n",
        "    print(\"--------------------------------------------------\")\n",
        "    #text=tokenizer.decode(i['labels'][0], skip_special_tokens=False)\n",
        "    #print(text)\n",
        "    #print(f\"\\r{c++}\",end=\"\")\n",
        "    print(i[\"text\"])\n",
        "    break"
      ],
      "metadata": {
        "id": "UhYqORflCB-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd359ad8-a3c6-4466-891c-05efe000a51e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "['<|startheader|>system:<|endheader|>You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.<|endoftext|><|startheader|>user:<|endheader|>Article: Communication is observed within the plant organism, i.e. within plant cells and between plant cells, between plants of the same or related species, and between plants and non-plant organisms, especially in the root zone. Plant roots communicate with rhizome bacteria, fungi, and insects within the soil. These interactions are governed by syntactic, pragmatic, and semantic rules,[citation needed] and are possible because of the decentralized \"nervous system\" of plants. The original meaning of the word \"neuron\" in Greek is \"vegetable fiber\" and recent research has shown that most of the microorganism plant communication processes are neuron-like. Plants also communicate via volatiles when exposed to herbivory attack behavior, thus warning neighboring plants. In parallel they produce other volatiles to attract parasites which attack these herbivores. In stress situations plants can overwrite the genomes they inherited from their parents and revert to that of their grand- or great-grandparents.[citation needed]\\n\\nQuestion: What is the meaning of the word neuron in Greek?<|endoftext|><|startheader|>assistant:<|endheader|>The meaning of the word \"neuron\" in Greek is \"vegetable fiber.\"<|endoftext|>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer.encode(input_text),skip_special_tokens=False)"
      ],
      "metadata": {
        "id": "NeA1UINJDGTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(50258)"
      ],
      "metadata": {
        "id": "8SkS18_tQg8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-wgDfYptegUG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}